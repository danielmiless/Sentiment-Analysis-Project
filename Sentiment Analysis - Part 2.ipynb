{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_tweets_incl_SW</th>\n",
       "      <th>cleaned_tweets_SW_removed</th>\n",
       "      <th>cleaned_tweets_SW_removed_len_gt2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "      <td>fingerprint pregnancy test android apps beautiful cute health igers iphoneonly iphonesia iphone</td>\n",
       "      <td>fingerprint pregnancy test android apps beautiful cute health igers iphoneonly iphonesia iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/</td>\n",
       "      <td>finally a transparant silicon case thanks to my uncle yay sony xperia s sonyexperias</td>\n",
       "      <td>finally transparant silicon case thanks uncle yay sony xperia sonyexperias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu</td>\n",
       "      <td>we love this would you go talk makememories unplug relax iphone smartphone wifi connect</td>\n",
       "      <td>love talk makememories unplug relax iphone smartphone wifi connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/</td>\n",
       "      <td>i am wired i know i am george i wa made that way iphone cute daventry home</td>\n",
       "      <td>wired know george way iphone cute daventry home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!</td>\n",
       "      <td>what amazing service apple will not even talk to me about a question i have unless i pay them for their stupid support</td>\n",
       "      <td>amazing service apple talk question unless pay stupid support</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      1   \n",
       "\n",
       "                                                                                                                cleaned_tweets_incl_SW  \\\n",
       "0     #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone   \n",
       "1  Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/   \n",
       "2          We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu   \n",
       "3                     I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/   \n",
       "4         What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!   \n",
       "\n",
       "                                                                                                 cleaned_tweets_SW_removed  \\\n",
       "0                         fingerprint pregnancy test android apps beautiful cute health igers iphoneonly iphonesia iphone    \n",
       "1                                    finally a transparant silicon case thanks to my uncle yay sony xperia s sonyexperias    \n",
       "2                                 we love this would you go talk makememories unplug relax iphone smartphone wifi connect    \n",
       "3                                              i am wired i know i am george i wa made that way iphone cute daventry home    \n",
       "4  what amazing service apple will not even talk to me about a question i have unless i pay them for their stupid support    \n",
       "\n",
       "                                                                  cleaned_tweets_SW_removed_len_gt2  \n",
       "0  fingerprint pregnancy test android apps beautiful cute health igers iphoneonly iphonesia iphone   \n",
       "1                       finally transparant silicon case thanks uncle yay sony xperia sonyexperias   \n",
       "2                               love talk makememories unplug relax iphone smartphone wifi connect   \n",
       "3                                                  wired know george way iphone cute daventry home   \n",
       "4                                    amazing service apple talk question unless pay stupid support   "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(\"cleaned_tweets.pkl\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fingerprint',\n",
       " 'pregnancy',\n",
       " 'test',\n",
       " 'android',\n",
       " 'apps',\n",
       " 'beautiful',\n",
       " 'cute',\n",
       " 'health',\n",
       " 'igers',\n",
       " 'iphoneonly',\n",
       " 'iphonesia',\n",
       " 'iphone']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_list = list(data['cleaned_tweets_SW_removed'].apply(lambda x: x.split()))\n",
    "tweets_list[0] # list of lists, where each tweet is a list of tokens, finally we have a list of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating your own Word2Vec Model & Train\n",
    "from gensim.models import Word2Vec\n",
    "# train model\n",
    "cbow_model = Word2Vec(tweets_list, vector_size = 300, window = 3, min_count=5, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=2440, vector_size=300, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded model\n",
    "print(cbow_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iphone',\n",
       " 'apple',\n",
       " 'i',\n",
       " 'my',\n",
       " 'the',\n",
       " 'to',\n",
       " 'a',\n",
       " 'is',\n",
       " 'samsung',\n",
       " 'it',\n",
       " 'and',\n",
       " 'you',\n",
       " 'new',\n",
       " 'twitter',\n",
       " 'for',\n",
       " 'com',\n",
       " 'phone',\n",
       " 'me',\n",
       " 'sony',\n",
       " 'not']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.index_to_key[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2440"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cbow_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc):\n",
    "    \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n",
    "    \n",
    "    # doc1 contains those words of the document which are included in the vocab\n",
    "    doc1 = [word for word in doc.split() if word in cbow_model.wv.index_to_key]\n",
    "    \n",
    "    wv1 = []  # this will contain the WE of all the vocab words from the doc\n",
    "    for word in doc1:\n",
    "        wv1.append(cbow_model.wv.get_vector(word))\n",
    "    wv1_ = np.array(wv1)\n",
    "    wv1_mean = wv1_.mean(axis=0)\n",
    "    return wv1_mean\n",
    "\n",
    "# np.mean(model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_temp = data['cleaned_tweets_SW_removed'].apply(document_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.0399835, 0.33625486, -0.02500192, 0.05551723, -0.02032441, -0.38807315, 0.15851666, 0.52045566, -0.124208905, 0.030367225, -0.03986065, -0.23179263, -0.04339204, -0.12538682, -0.1452291, -0.152...\n",
       "1    [0.013774453, 0.20576945, 0.033048704, 0.055075806, -0.012772152, -0.31885657, 0.23940869, 0.39691025, -0.009999564, -0.17538038, -0.029512888, -0.2500132, 0.011852333, 0.08854894, -0.17654714, -0...\n",
       "2    [0.033355672, 0.15187229, 0.085422955, 0.124750316, 0.009809439, -0.20016758, 0.2016915, 0.4177847, 0.099185616, -0.112650305, -0.014595959, -0.23172669, 0.013462442, 0.041303895, -0.15793206, -0....\n",
       "3    [0.08745195, 0.13059999, 0.14677054, 0.18138403, 0.013174023, -0.17348404, 0.2313592, 0.46672985, 0.16958979, -0.1478469, 0.0079227565, -0.2903385, 0.03146164, 0.10342252, -0.18295807, 0.005532910...\n",
       "4    [0.057071388, 0.124738224, 0.10685496, 0.15387082, 0.012065107, -0.18567508, 0.22289039, 0.4319276, 0.1582634, -0.17510347, 0.000743964, -0.26172107, 0.013467538, 0.1046397, -0.18482377, -0.049187...\n",
       "Name: cleaned_tweets_SW_removed, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_temp[:5]  # displaying the 1st 5 tweets, as document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_temp[0].shape  # each document vecotr is 300-dimensional !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweets_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 300)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining all the document vectors into a singl numpy array (tweets_vec)\n",
    "embedding_size = 300\n",
    "tweets_vec = np.ones((len(tweets_temp), embedding_size))*np.nan\n",
    "for i in range(tweets_vec.shape[0]):\n",
    "    tweets_vec[i,:] = tweets_temp.iloc[i]\n",
    "\n",
    "tweets_vec.shape # this itself is your final FEATURE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DF to store these new documnent features\n",
    "df = pd.DataFrame(tweets_vec)\n",
    "df['y'] = data['label']\n",
    "df.dropna(how='any', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.039983</td>\n",
       "      <td>0.336255</td>\n",
       "      <td>-0.025002</td>\n",
       "      <td>0.055517</td>\n",
       "      <td>-0.020324</td>\n",
       "      <td>-0.388073</td>\n",
       "      <td>0.158517</td>\n",
       "      <td>0.520456</td>\n",
       "      <td>-0.124209</td>\n",
       "      <td>0.030367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215388</td>\n",
       "      <td>0.201467</td>\n",
       "      <td>-0.033585</td>\n",
       "      <td>0.260728</td>\n",
       "      <td>0.252156</td>\n",
       "      <td>0.082151</td>\n",
       "      <td>-0.109638</td>\n",
       "      <td>0.117947</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.205769</td>\n",
       "      <td>0.033049</td>\n",
       "      <td>0.055076</td>\n",
       "      <td>-0.012772</td>\n",
       "      <td>-0.318857</td>\n",
       "      <td>0.239409</td>\n",
       "      <td>0.396910</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.175380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263607</td>\n",
       "      <td>0.244832</td>\n",
       "      <td>-0.086145</td>\n",
       "      <td>0.204839</td>\n",
       "      <td>0.318631</td>\n",
       "      <td>-0.051588</td>\n",
       "      <td>-0.244891</td>\n",
       "      <td>0.097621</td>\n",
       "      <td>-0.084559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033356</td>\n",
       "      <td>0.151872</td>\n",
       "      <td>0.085423</td>\n",
       "      <td>0.124750</td>\n",
       "      <td>0.009809</td>\n",
       "      <td>-0.200168</td>\n",
       "      <td>0.201691</td>\n",
       "      <td>0.417785</td>\n",
       "      <td>0.099186</td>\n",
       "      <td>-0.112650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236323</td>\n",
       "      <td>0.217120</td>\n",
       "      <td>-0.017093</td>\n",
       "      <td>0.200741</td>\n",
       "      <td>0.249649</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.116789</td>\n",
       "      <td>0.077153</td>\n",
       "      <td>-0.078748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.087452</td>\n",
       "      <td>0.130600</td>\n",
       "      <td>0.146771</td>\n",
       "      <td>0.181384</td>\n",
       "      <td>0.013174</td>\n",
       "      <td>-0.173484</td>\n",
       "      <td>0.231359</td>\n",
       "      <td>0.466730</td>\n",
       "      <td>0.169590</td>\n",
       "      <td>-0.147847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273778</td>\n",
       "      <td>0.271048</td>\n",
       "      <td>0.005677</td>\n",
       "      <td>0.243268</td>\n",
       "      <td>0.244216</td>\n",
       "      <td>0.011721</td>\n",
       "      <td>-0.099318</td>\n",
       "      <td>0.087006</td>\n",
       "      <td>-0.078905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.057071</td>\n",
       "      <td>0.124738</td>\n",
       "      <td>0.106855</td>\n",
       "      <td>0.153871</td>\n",
       "      <td>0.012065</td>\n",
       "      <td>-0.185675</td>\n",
       "      <td>0.222890</td>\n",
       "      <td>0.431928</td>\n",
       "      <td>0.158263</td>\n",
       "      <td>-0.175103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269000</td>\n",
       "      <td>0.261812</td>\n",
       "      <td>-0.019221</td>\n",
       "      <td>0.216414</td>\n",
       "      <td>0.284855</td>\n",
       "      <td>-0.017990</td>\n",
       "      <td>-0.132647</td>\n",
       "      <td>0.068499</td>\n",
       "      <td>-0.106739</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.039983  0.336255 -0.025002  0.055517 -0.020324 -0.388073  0.158517   \n",
       "1  0.013774  0.205769  0.033049  0.055076 -0.012772 -0.318857  0.239409   \n",
       "2  0.033356  0.151872  0.085423  0.124750  0.009809 -0.200168  0.201691   \n",
       "3  0.087452  0.130600  0.146771  0.181384  0.013174 -0.173484  0.231359   \n",
       "4  0.057071  0.124738  0.106855  0.153871  0.012065 -0.185675  0.222890   \n",
       "\n",
       "          7         8         9  ...       291       292       293       294  \\\n",
       "0  0.520456 -0.124209  0.030367  ...  0.215388  0.201467 -0.033585  0.260728   \n",
       "1  0.396910 -0.010000 -0.175380  ...  0.263607  0.244832 -0.086145  0.204839   \n",
       "2  0.417785  0.099186 -0.112650  ...  0.236323  0.217120 -0.017093  0.200741   \n",
       "3  0.466730  0.169590 -0.147847  ...  0.273778  0.271048  0.005677  0.243268   \n",
       "4  0.431928  0.158263 -0.175103  ...  0.269000  0.261812 -0.019221  0.216414   \n",
       "\n",
       "        295       296       297       298       299  y  \n",
       "0  0.252156  0.082151 -0.109638  0.117947  0.006738  0  \n",
       "1  0.318631 -0.051588 -0.244891  0.097621 -0.084559  0  \n",
       "2  0.249649 -0.000079 -0.116789  0.077153 -0.078748  0  \n",
       "3  0.244216  0.011721 -0.099318  0.087006 -0.078905  0  \n",
       "4  0.284855 -0.017990 -0.132647  0.068499 -0.106739  1  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 301)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 300)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_word_emb = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "X_word_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.61 0.29\n",
      "85.34 0.89\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "LR1 = LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1', C=0.4, random_state=42)\n",
    "WE_pipe = Pipeline([('SC', StandardScaler()), ('LR', LR1)] )\n",
    "\n",
    "results = cross_validate(WE_pipe, X_word_emb, y, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# print(results['train_score'])\n",
    "print(np.round((results['train_score'].mean())*100, 2), np.round((results['train_score'].std())*100, 2)) \n",
    "\n",
    "# print(results['test_score'])\n",
    "print(np.round((results['test_score'].mean())*100, 2), np.round((results['test_score'].std())*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.96 0.1\n",
      "87.78 0.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['cleaned_tweets_SW_removed']\n",
    "y = data['label']\n",
    "\n",
    "# we want to include only those words in the vocab which have min df of 5,\n",
    "# means select only those words which occur ATLEAST in 5 documents!! \n",
    "# AND SELECT the TOP 300 FEATURES ONLY to build the model\n",
    "CV = CountVectorizer(min_df=5, max_features=300)\n",
    "\n",
    "LR1 = LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1', C=0.4)\n",
    "CV_pipe = Pipeline([('CV', CV) , ('LR', LR1)] )\n",
    "results = cross_validate(CV_pipe, X, y, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# print(results['train_score'])\n",
    "print(np.round((results['train_score'].mean())*100, 2), np.round((results['train_score'].std())*100, 2)) \n",
    "\n",
    "# print(results['test_score'])\n",
    "print(np.round((results['test_score'].mean())*100, 2), np.round((results['test_score'].std())*100, 2)) \n",
    "\n",
    "CV.fit_transform(X)\n",
    "len(CV.vocabulary_)  # no. of features AFTER applying the stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings from GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'word2vec.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment Analysis - Part 2.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load the converted model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mword2vec.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m KeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(filename, binary\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[1;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[1;32m   1722\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gensim/models/keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2045\u001b[0m             counts[word] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(count)\n\u001b[1;32m   2047\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading projection weights from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, fname)\n\u001b[0;32m-> 2048\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[1;32m   2049\u001b[0m     \u001b[39mif\u001b[39;00m no_header:\n\u001b[1;32m   2050\u001b[0m         \u001b[39m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[39mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/smart_open/smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 177\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[1;32m    178\u001b[0m     uri,\n\u001b[1;32m    179\u001b[0m     mode,\n\u001b[1;32m    180\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    181\u001b[0m     buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[1;32m    182\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    183\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    184\u001b[0m     newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[1;32m    185\u001b[0m )\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/smart_open/smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    361\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39;49mbuffering, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'word2vec.txt'"
     ]
    }
   ],
   "source": [
    "# load the converted model\n",
    "filename = 'word2vec.txt'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment Analysis - Part 2.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mget_vector(\u001b[39m'\u001b[39m\u001b[39manalytics\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.get_vector('analytics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector_GloVe(doc):\n",
    "    \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n",
    "    \n",
    "    # doc1 contains those words of the document which are included in the vocab\n",
    "    doc1 = [word for word in doc.split() if word in cbow_model.wv.index_to_key]\n",
    "    \n",
    "    wv1 = []  # this will contain the WE of all the vocab words from the doc\n",
    "    for word in doc1:\n",
    "        wv1.append(model.get_vector(word))\n",
    "    wv1_ = np.array(wv1)\n",
    "    wv1_mean = wv1_.mean(axis=0)\n",
    "    return wv1_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment Analysis - Part 2.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tweets_temp \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39mcleaned_tweets_SW_removed\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(document_vector_GloVe)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4751\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4752\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4753\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4754\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   4755\u001b[0m         func,\n\u001b[1;32m   4756\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[1;32m   4757\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[1;32m   4758\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   4759\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[0;32m-> 4760\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[1;32m   1206\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[1;32m   1288\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[1;32m   1289\u001b[0m )\n\u001b[1;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1292\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2920\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment Analysis - Part 2.ipynb Cell 28\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m wv1 \u001b[39m=\u001b[39m []  \u001b[39m# this will contain the WE of all the vocab words from the doc\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m doc1:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     wv1\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39mget_vector(word))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m wv1_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(wv1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m wv1_mean \u001b[39m=\u001b[39m wv1_\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_temp = data['cleaned_tweets_SW_removed'].apply(document_vector_GloVe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.0076073036, 0.36563912, -0.017829334, 0.13810664, 0.002734739, -0.37158328, 0.12625904, 0.5248367, -0.17825066, 0.01851082, -0.06740156, -0.16306688, -0.059494246, -0.062954575, -0.16027455, -0...\n",
       "1    [0.0158105, 0.20234677, 0.06099586, 0.07994256, 0.007336532, -0.29250187, 0.23279652, 0.36627945, 0.010631448, -0.14495794, 0.006304062, -0.2803884, -0.015781898, 0.10213733, -0.15599898, -0.24977...\n",
       "2    [0.0400173, 0.13644567, 0.10470885, 0.11657105, 0.02667524, -0.18809159, 0.21864848, 0.38404962, 0.124423295, -0.09046478, 0.02517883, -0.26747122, -0.0029446005, 0.039209347, -0.15143086, -0.0922...\n",
       "3    [0.09637585, 0.1071649, 0.15986662, 0.16335377, 0.019727113, -0.17757939, 0.27743855, 0.41078225, 0.19750166, -0.11732262, 0.06421344, -0.34289852, -0.006108156, 0.08522984, -0.17811987, -0.076994...\n",
       "4    [0.0717861, 0.09557765, 0.12791885, 0.12585501, 0.024032945, -0.17412983, 0.25427285, 0.37239587, 0.19065553, -0.13509518, 0.05637974, -0.32273, -0.011662999, 0.082977675, -0.16600677, -0.1104085,...\n",
       "Name: cleaned_tweets_SW_removed, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_temp[:5]  # displaying the 1st 5 tweets, as document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (300,) into shape (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment Analysis - Part 2.ipynb Cell 30\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tweets_vec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39mlen\u001b[39m(tweets_temp), embedding_size))\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mnan\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(tweets_vec\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     tweets_vec[i,:] \u001b[39m=\u001b[39m tweets_temp\u001b[39m.\u001b[39miloc[i]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# tweets_vec.shape # this itself is your final FEATURE MATRIX\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Create a new DF to store these new documnent features\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danielmiles/Documents/Python/Sentiment-Analysis-Project/Sentiment%20Analysis%20-%20Part%202.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m df1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(tweets_vec)\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (300,) into shape (100,)"
     ]
    }
   ],
   "source": [
    "# Combining all the document vectors into a singl numpy array (tweets_vec)\n",
    "embedding_size = 100\n",
    "tweets_vec = np.ones((len(tweets_temp), embedding_size))*np.nan\n",
    "for i in range(tweets_vec.shape[0]):\n",
    "    tweets_vec[i,:] = tweets_temp.iloc[i]\n",
    "\n",
    "# tweets_vec.shape # this itself is your final FEATURE MATRIX\n",
    "# Create a new DF to store these new documnent features\n",
    "df1 = pd.DataFrame(tweets_vec)\n",
    "df1['y'] = data['label']\n",
    "df1.dropna(how='any', axis=0, inplace=True)\n",
    "\n",
    "X_word_emb = df1.drop('y', axis=1)\n",
    "y = df1['y']\n",
    "X_word_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.58 0.26\n",
      "85.27 0.49\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "LR1 = LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1', C=0.4, random_state=42)\n",
    "WE_pipe = Pipeline([('SC', StandardScaler()), ('LR', LR1)] )\n",
    "\n",
    "results = cross_validate(WE_pipe, X_word_emb, y, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# print(results['train_score'])\n",
    "print(np.round((results['train_score'].mean())*100, 2), np.round((results['train_score'].std())*100, 2)) \n",
    "\n",
    "# print(results['test_score'])\n",
    "print(np.round((results['test_score'].mean())*100, 2), np.round((results['test_score'].std())*100, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
